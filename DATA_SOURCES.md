# Data Sources for Measuring AI Penetration in Software Engineering

A comprehensive catalog of public repositories, datasets, APIs, and data sources that can provide quantitative signals about AI's role in software development. Organized by category with details on content, usage, access, and status.

Last updated: 2026-02-07

---

## Table of Contents

1. [Code Hosting Platforms & Their APIs](#1-code-hosting-platforms--their-apis)
2. [Git Archive & Historical Code Datasets](#2-git-archive--historical-code-datasets)
3. [Package Registries & Dependency Data](#3-package-registries--dependency-data)
4. [Developer Surveys & Industry Reports](#4-developer-surveys--industry-reports)
5. [Open Source Analytics Platforms](#5-open-source-analytics-platforms)
6. [AI Code Detection & Attribution](#6-ai-code-detection--attribution)
7. [Academic Datasets & Benchmarks](#7-academic-datasets--benchmarks)
8. [Code Quality & CI/CD Metrics](#8-code-quality--cicd-metrics)
9. [Developer Community & Sentiment Data](#9-developer-community--sentiment-data)
10. [Job Market & Skills Data](#10-job-market--skills-data)
11. [Web Archives & Trend Data](#11-web-archives--trend-data)
12. [Security & Vulnerability Data](#12-security--vulnerability-data)
13. [AI Tool Vendor Metrics](#13-ai-tool-vendor-metrics)

---

## 1. Code Hosting Platforms & Their APIs

### 1.1 GitHub Search API (Commits)

- **URL**: https://docs.github.com/en/rest/search/search#search-commits
- **What it contains**: Search across all public GitHub commits by message content, author, date, repository, etc.
- **How to measure AI**: Search for AI attribution strings in commit messages (e.g., "Co-Authored-By: Claude", "Generated by Copilot", "aider"). This is the primary method used by this project.
- **Access**: Free with authentication. 30 requests/minute for authenticated users; 10 for unauthenticated. Returns total_count but only 1,000 results per query.
- **Status**: Active. This is the core data source for the claude-code-tracker project.

### 1.2 GitHub REST & GraphQL APIs

- **URL**: https://docs.github.com/en/rest | https://docs.github.com/en/graphql
- **What it contains**: Repositories, pull requests, issues, commits, users, organizations, actions, code scanning, dependency graph, etc.
- **How to measure AI**: Pull request bodies/descriptions mentioning AI tools, PR labels (e.g., "copilot"), bot accounts creating PRs (Copilot coding agent, Devin, etc.), Copilot usage metrics API (for org admins).
- **Access**: Free with token. Primary rate limit: 5,000 requests/hour (authenticated). GraphQL: 5,000 points/hour. Some endpoints (Copilot metrics) require org admin access.
- **Status**: Active.

### 1.3 GitHub Copilot Usage Metrics API

- **URL**: https://docs.github.com/en/copilot/concepts/copilot-metrics
- **What it contains**: Copilot code completion acceptance rates, lines suggested/accepted, active users, language breakdown, IDE breakdown, agent PR counts (~1.2M PRs/month as of 2025).
- **How to measure AI**: Direct measurement of Copilot adoption and code generation volume within organizations.
- **Access**: Requires GitHub Enterprise or Organization admin. Public preview as of Oct 2025. NDJSON export available.
- **Status**: Active (public preview).

### 1.4 GitHub Innovation Graph

- **URL**: https://innovationgraph.github.com
- **What it contains**: Quarterly data on git pushes, developer counts, organizations, repositories, programming languages, licenses, and topics by economy/country, dating back to 2020.
- **How to measure AI**: Track language shifts (e.g., TypeScript overtaking Python), repository creation rates, and push volume trends that correlate with AI tool adoption. Data shows 1.1M+ public repos importing LLM SDKs (up 178% YoY).
- **Access**: Free. Data downloadable as CSV under CC0-1.0 license.
- **Status**: Active. Updated quarterly.

### 1.5 GitLab REST API

- **URL**: https://docs.gitlab.com/api/api_resources/
- **What it contains**: Project statistics, repository analytics, commit data, merge requests, issues, CI/CD pipelines.
- **How to measure AI**: Search merge request descriptions and commit messages for AI tool signatures; analyze CI/CD pipeline data for AI-assisted workflows; track repository analytics.
- **Access**: Free for public projects. Authentication required. Rate limit: 2,000 requests/minute for authenticated users.
- **Status**: Active.

### 1.6 Bitbucket Cloud REST API

- **URL**: https://developer.atlassian.com/cloud/bitbucket/rest/api-group-repositories/
- **What it contains**: Repositories, pull requests, commits, pipelines, code review data.
- **How to measure AI**: Search commit messages and PR descriptions for AI attribution; track bot accounts.
- **Access**: Free for public repositories. Authentication required for private. Rate limit: 1,000 requests/hour.
- **Status**: Active.

### 1.7 Codeberg / Forgejo API

- **URL**: https://codeberg.org/api/swagger
- **What it contains**: Repository data, commits, issues, pull requests for Codeberg's open-source-focused hosting platform (powered by Forgejo, a Gitea fork).
- **How to measure AI**: Search commits and PRs for AI attribution in a community that trends toward FOSS-only and may have different AI adoption patterns than GitHub.
- **Access**: Free. Gitea-compatible API.
- **Status**: Active. Smaller but growing community focused on software freedom.

---

## 2. Git Archive & Historical Code Datasets

### 2.1 GH Archive

- **URL**: https://www.gharchive.org/
- **What it contains**: Hourly archives of all public GitHub events since 2011. Stored in Google BigQuery (`githubarchive.day.*` and `githubarchive.month.*` tables).
- **How to measure AI**: Query PushEvent payloads for commit messages containing AI signatures. **CAVEAT**: GitHub stripped commit messages from PushEvent payloads around October 2025, making this source unreliable for commit-message-based analysis after that date. Still useful for event volume, repository creation trends, and pre-Oct-2025 historical analysis.
- **Access**: Free via BigQuery (1 TB free/month query processing). Raw JSON files downloadable from gharchive.org.
- **Status**: Active but degraded for commit-level analysis due to GitHub's payload changes.

### 2.2 GHTorrent

- **URL**: https://ghtorrent.org/
- **What it contains**: Offline mirror of GitHub API event streams. MongoDB (raw data) + MySQL (relational). Includes commits, PRs, issues, users, organizations, watchers, forks.
- **How to measure AI**: Historical analysis of commit patterns, bot account identification, collaboration network analysis. Contains metadata not available in GH Archive.
- **Access**: Free for research. MySQL dumps and MongoDB dumps available. Used extensively in MSR (Mining Software Repositories) research.
- **Status**: Partially maintained. Last major dumps are from earlier years; may not have current data.

### 2.3 Software Heritage Archive

- **URL**: https://www.softwareheritage.org/
- **What it contains**: The largest public archive of software source code and development history: 5+ billion unique source code files, 1+ billion unique commits, 80+ million software projects. Sources include GitHub, GitLab, Debian, PyPI, CRAN, and more.
- **How to measure AI**: Analyze actual source code content (not just metadata) for AI-generated patterns. Cross-reference code across platforms. Used as the basis for The Stack v2 dataset.
- **Access**: Free. API available. Graph dataset available for research. Bulk access via Software Heritage Graph Dataset on Amazon S3.
- **Status**: Active. Continuously updated.

### 2.4 World of Code (WoC)

- **URL**: https://worldofcode.org/
- **What it contains**: Universal collection of FLOSS version control data. 18+ billion Git objects. Cross-references authors, projects, commits, blobs, dependencies across all of open source. Monthly updates.
- **How to measure AI**: Track code flow and sharing patterns across the entire OSS ecosystem. Detect AI-generated code patterns at scale. Measure dependency adoption of AI libraries.
- **Access**: Free for academic research. Requires account. Hosted at University of Tennessee.
- **Status**: Active. Research infrastructure maintained by academic team.

### 2.5 Public Git Archive

- **URL**: https://github.com/src-d/datasets/tree/master/PublicGitArchive
- **What it contains**: Dataset of 182,014 top-starred GitHub repositories (3.0 TB on disk) including full git history and source code content.
- **How to measure AI**: Analyze actual code content for AI-generated patterns. Compare code quality and style metrics over time.
- **Access**: Free. Downloadable from GCS.
- **Status**: Dataset is static (snapshot); source{d} (the company) is defunct, but the data remains available.

### 2.6 Google BigQuery: GitHub Public Dataset

- **URL**: https://cloud.google.com/blog/topics/public-datasets/github-on-bigquery-analyze-all-the-open-source-code
- **What it contains**: Full snapshot of 2.8+ million open source GitHub repositories. Contents table has all non-binary files under 1MB (1.5+ TB). Also includes commits, files, languages, and licenses tables.
- **How to measure AI**: Analyze source code for AI-generated patterns, import statements for AI libraries, license changes, and code quality metrics at massive scale.
- **Access**: Free (1 TB/month query processing). BigQuery table: `bigquery-public-data.github_repos.*`
- **Status**: Active. Periodic snapshots.

---

## 3. Package Registries & Dependency Data

### 3.1 npm Registry API & Download Counts

- **URL**: https://github.com/npm/registry/blob/main/docs/download-counts.md
- **What it contains**: Download statistics for all npm packages. Point and range endpoints. Bulk queries up to 128 packages. Data from Jan 10, 2015 onward. Up to 18 months of history.
- **How to measure AI**: Track download trends for AI/LLM-related packages (langchain, openai, @anthropic-ai/sdk, ai, etc.) as a proxy for AI integration in JS/TS projects.
- **Access**: Free. No authentication required. Endpoint: `https://api.npmjs.org/downloads/`
- **Status**: Active.

### 3.2 PyPI BigQuery Dataset (Python Package Downloads)

- **URL**: https://docs.pypi.org/api/bigquery/ | https://packaging.python.org/guides/analyzing-pypi-package-downloads/
- **What it contains**: Streaming download logs for all PyPI packages in BigQuery table `bigquery-public-data.pypi.file_downloads`. Includes package name, version, installer, Python version, OS, country, timestamp.
- **How to measure AI**: Track downloads of AI/ML packages (openai, anthropic, langchain, transformers, torch, etc.) over time. Measure growth of AI library adoption in the Python ecosystem.
- **Access**: Free via BigQuery (1 TB/month free). Also available via pypistats.org JSON API and `pypinfo` CLI tool.
- **Status**: Active. Continuously updated via Linehaul streaming.

### 3.3 crates.io (Rust Package Registry)

- **URL**: https://crates.io/ | API: https://crates.io/api/v1/
- **What it contains**: Rust package metadata, download counts, version history, dependencies. Also provides daily database dumps and the crates.io-index Git repository.
- **How to measure AI**: Track AI-related Rust crates (llm, candle, etc.) and overall ecosystem growth.
- **Access**: Free. API with rate limiting. Database dumps available.
- **Status**: Active.

### 3.4 Libraries.io

- **URL**: https://libraries.io/ | API: https://libraries.io/api
- **What it contains**: Metadata for 9.96M+ packages across 36 package managers (npm, Maven, PyPI, NuGet, etc.). Includes versions, dependencies (100M+ repository dependencies), contributors, licenses, and SourceRank scores.
- **How to measure AI**: Track cross-ecosystem adoption of AI libraries. Analyze dependency graphs to see how deeply AI packages have penetrated the software supply chain. Measure AI library SourceRank and adoption velocity.
- **Access**: Free API with 60 requests/minute rate limit (API key required). Full dataset on Zenodo under CC BY-SA-4.0. Also available on Kaggle.
- **Status**: Active.

### 3.5 deps.dev (Google Open Source Insights)

- **URL**: https://deps.dev/ | API: https://docs.deps.dev/api/v3alpha/
- **What it contains**: Dependency metadata, security advisories, licenses, and health signals for 50M+ package versions across Cargo, Go, Maven, npm, NuGet, PyPI, and RubyGems. Includes unique hash-based file lookup.
- **How to measure AI**: Analyze dependency trees for AI library adoption across multiple ecosystems simultaneously. Track security advisories for AI-related packages. Cross-reference with vulnerability data.
- **Access**: Free. JSON/HTTP and gRPC APIs. Also available as BigQuery public dataset.
- **Status**: Active. Maintained by Google.

### 3.6 Repology

- **URL**: https://repology.org/ | API: https://repology.org/api
- **What it contains**: Tracks packaging status of software across 340+ repositories/distributions (Debian, Fedora, Homebrew, etc.). Cross-references package versions across ecosystems.
- **How to measure AI**: Track how AI tools and libraries are being packaged and distributed across Linux distributions and package managers.
- **Access**: Free API.
- **Status**: Active.

---

## 4. Developer Surveys & Industry Reports

### 4.1 Stack Overflow Annual Developer Survey

- **URL**: https://survey.stackoverflow.co/2025 | Raw data: https://survey.stackoverflow.co/
- **What it contains**: 49,000+ responses from 177 countries (2025). Covers AI tool usage, programming languages, frameworks, developer demographics, salary, job satisfaction. AI-specific sections on tool adoption (80% using AI tools), trust (29% trust accuracy), LLM preferences (GPT 82%, Claude Sonnet most admired).
- **How to measure AI**: Direct survey data on AI tool adoption rates, trust levels, usage patterns, frustrations (45% cite "almost right" solutions), and which LLMs developers prefer.
- **Access**: Free. Raw survey data downloadable. Published annually since 2011.
- **Status**: Active. 2025 edition released Dec 2025.

### 4.2 JetBrains State of Developer Ecosystem

- **URL**: https://devecosystem-2025.jetbrains.com | AI section: https://devecosystem-2025.jetbrains.com/artificial-intelligence
- **What it contains**: 24,534 respondents from 194 countries (2025). Covers AI tool usage (85% regular users), tool preferences (ChatGPT 41%, Copilot 30%), benefits, concerns, programming languages, frameworks, testing practices.
- **How to measure AI**: Most granular survey data on AI coding tool adoption, with breakdowns by language, experience level, and geography. Reports that 62% rely on at least one AI coding assistant/agent.
- **Access**: Free. Detailed interactive report with data visualizations. Raw data not publicly released but summary statistics are comprehensive.
- **Status**: Active. Published annually.

### 4.3 GitHub Octoverse Report

- **URL**: https://octoverse.github.com/
- **What it contains**: Annual report on GitHub platform activity. 2025 data: 180M+ developers, 36M new in past year, 43.2M PRs merged monthly (+23% YoY), nearly 1B commits pushed (+25.1% YoY). TypeScript overtook Python as #1 language. 80% of new devs used Copilot in first week.
- **How to measure AI**: Track macro trends in code volume, language shifts, PR velocity, and explicit Copilot adoption metrics. Data on 1.1M+ repos importing LLM SDKs (up 178% YoY).
- **Access**: Free report. Some underlying data available via GitHub Innovation Graph.
- **Status**: Active. Published annually.

### 4.4 DORA Accelerate State of DevOps Report

- **URL**: https://dora.dev/research/2025/dora-report/
- **What it contains**: 2025 report titled "State of AI-assisted Software Development." Based on survey data measuring software delivery metrics (lead time, deployment frequency, change failure rate, recovery time, rework rate). 90% respondents using AI daily; 65% heavily reliant.
- **How to measure AI**: The definitive source linking AI adoption to delivery performance. Finding: AI boosts individual output (21% more tasks, 98% more PRs) but organizational delivery metrics stay flat. Introduces 7 team archetypes based on AI adoption patterns.
- **Access**: Free report PDF. Raw data not publicly available but methodology is published.
- **Status**: Active. Published annually by Google DORA team.

### 4.5 Postman State of the API Report

- **URL**: https://www.postman.com/state-of-api/2025/
- **What it contains**: 5,700+ respondents. API usage patterns, development practices, AI adoption (89% use generative AI daily; ChatGPT 69%, Copilot 58%). Only 24% design APIs for AI agents. 51% of orgs have deployed AI agents.
- **How to measure AI**: Tracks AI adoption specifically in API development context. Shows how API-first development intersects with AI tool usage.
- **Access**: Free report and PDF download.
- **Status**: Active. Published annually.

### 4.6 METR Randomized Controlled Trial (AI + Developer Productivity)

- **URL**: https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/ | Paper: https://arxiv.org/abs/2507.09089
- **What it contains**: Rigorous RCT with 16 experienced OSS developers completing 246 tasks on mature projects (5+ years experience each). Used Cursor Pro + Claude 3.5/3.7 Sonnet. Measured actual task completion times.
- **How to measure AI**: One of the few controlled experiments measuring real AI productivity impact. Provides ground-truth data on actual (vs. perceived) productivity changes.
- **Access**: Free. Paper and methodology published.
- **Status**: Published July 2025. One-time study.

### 4.7 GitClear Code Quality Research Reports

- **URL**: https://www.gitclear.com/coding_on_copilot_data_shows_ais_downward_pressure_on_code_quality | https://www.gitclear.com/ai_assistant_code_quality_2025_research
- **What it contains**: Analysis of 153M+ changed lines of code (2020-2023) and 211M+ lines (2020-2024). Measures code churn, copy/paste rates, refactoring trends. Finds code churn doubled from pre-AI baseline; code clones grew 4x; refactoring declined from 25% to <10% of changes.
- **How to measure AI**: Quantitative evidence of AI's impact on code quality metrics. Tracks "moved code" vs "added code" vs "copy/pasted code" ratios as proxies for AI-assisted development patterns.
- **Access**: Free summary reports. Full methodology published. Underlying data is proprietary (aggregated from GitClear customers).
- **Status**: Active. Updated annually.

### 4.8 Jellyfish Engineering Metrics Reports

- **URL**: https://jellyfish.co/blog/2025-ai-metrics-in-review/
- **What it contains**: Aggregated engineering metrics from tens of thousands of users across hundreds of organizations. Reports that ~50% of companies have at least 50% AI-generated code (up from 20% at year start). Only 20% of teams use metrics to measure AI impact.
- **How to measure AI**: Provides organizational-level adoption data and the gap between AI investment and accountability.
- **Access**: Free summary reports. Underlying data is proprietary.
- **Status**: Active.

---

## 5. Open Source Analytics Platforms

### 5.1 OSS Insight

- **URL**: https://ossinsight.io/
- **What it contains**: Real-time analytics for any GitHub repository or developer, powered by TiDB. Monthly and historical rankings. Built on GH Archive (4.6B+ event rows). Provides trending repositories, developer comparisons, and collection-based analytics.
- **How to measure AI**: Track growth of AI-related repositories, contributor patterns for AI projects, star/fork velocity of AI tools, and language ecosystem shifts.
- **Access**: Free. Web interface and some API access.
- **Status**: Active. Maintained by PingCAP/TiDB team.

### 5.2 CNCF DevStats

- **URL**: https://devstats.cncf.io/ | Source: https://github.com/cncf/devstats
- **What it contains**: Grafana dashboards visualizing contributor activity, PR velocity, issue resolution times, company affiliations, and more for all CNCF projects (Kubernetes, Prometheus, etc.). Built on GH Archive + GitHub API.
- **How to measure AI**: Track how AI adoption affects contribution patterns to major infrastructure projects. Measure bot vs. human contributions. Analyze company participation trends.
- **Access**: Free. Fully open source. Grafana dashboards publicly accessible.
- **Status**: Active. Maintained by CNCF.

### 5.3 OpenDigger

- **URL**: https://open-digger.cn/en/
- **What it contains**: Implements CHAOSS community metrics plus custom metrics like OpenRank. Exports all metrics as separate files on cloud storage for anyone to use. Covers thousands of open-source projects.
- **How to measure AI**: Use standardized open-source health metrics to compare AI-era projects vs. pre-AI baselines. Track contribution patterns, community health, and project velocity.
- **Access**: Free. Metrics data exported as public files.
- **Status**: Active. Developed by X-lab.

### 5.4 GrimoireLab (CHAOSS)

- **URL**: https://chaoss.github.io/grimoirelab/ | https://github.com/chaoss/grimoirelab
- **What it contains**: Software development analytics platform. Collects data from 30+ sources (Git, GitHub, GitLab, Jira, Slack, mailing lists, etc.). Generates 150+ metrics and visualizations covering activity, performance, community health.
- **How to measure AI**: Self-hostable analytics that can be configured to track AI-specific metrics across multiple data sources. Can detect bot contributions, measure community composition changes, and track tool adoption.
- **Access**: Free. Fully open source (GPLv3). Part of Linux Foundation CHAOSS project.
- **Status**: Active. GrimoireLab 1.0 released.

### 5.5 Augur

- **URL**: https://github.com/chaoss/augur
- **What it contains**: Another CHAOSS analytics tool focused on open-source software health and sustainability metrics. Provides both a Python library and REST API.
- **How to measure AI**: Programmatic access to health metrics that can be compared before/after AI tool adoption.
- **Access**: Free. Open source.
- **Status**: Active.

---

## 6. AI Code Detection & Attribution

### 6.1 Git Co-Author Trailers (De Facto Standard)

- **URL**: N/A (git convention)
- **What it contains**: Git commit trailers like `Co-Authored-By: Claude <noreply@anthropic.com>`, `Co-Authored-By: Copilot <copilot@github.com>`, or aider's `(aider)` author name annotation.
- **How to measure AI**: Search commit messages for these trailers via GitHub Search API or BigQuery. Claude Code and some other tools add these automatically. This is the primary signal the claude-code-tracker uses.
- **Access**: Free. Searchable via GitHub Search API.
- **Status**: Emerging convention. Not universally adopted. Claude Code adds it automatically; Copilot does not add it automatically for completions (only for coding agent); Cursor requires manual configuration.

### 6.2 SonarQube AI Code Detection

- **URL**: https://www.sonarsource.com/blog/auto-detect-and-review-ai-generated-code-from-github-copilot/
- **What it contains**: SonarQube can automatically detect Copilot usage and mark projects with a "CONTAINS AI CODE" badge. Analyzes code patterns associated with AI generation.
- **How to measure AI**: Automated detection of AI-generated code in codebases, independent of commit message attribution. Could provide ground truth for validation.
- **Access**: SonarQube Community Edition is free; advanced features are paid.
- **Status**: Active. Feature enabled by default.

### 6.3 GPTSniffer / AI Code Detectors

- **URL**: Paper: https://www.sciencedirect.com/science/article/pii/S0164121224001043 | Tools: https://aicodedetector.org/
- **What it contains**: CodeBERT-based classifiers that distinguish human-written from AI-generated code. GPTSniffer outperforms GPTZero and OpenAI Text Classifier on code. Various online tools claim 90%+ accuracy for Python, JavaScript, PHP, C/C++, Java.
- **How to measure AI**: Classify source code samples as human or AI-generated. Could be applied at scale to open source repositories for estimation studies.
- **Access**: GPTSniffer is a research model. Online detectors are generally free for limited use.
- **Status**: Active research area. Accuracy varies; concerns about false positives remain.

### 6.4 aboutcode-org/ai-gen-code-search

- **URL**: https://github.com/aboutcode-org/ai-gen-code-search
- **What it contains**: Open-source utilities and tools to detect and search for AI-generated code using code fragment approximate similarity search against FOSS training data.
- **How to measure AI**: Identifies AI-generated code that may have been derived from existing open-source code. Useful for both detection and license compliance analysis.
- **Access**: Free. Open source.
- **Status**: Active.

### 6.5 Bot Detection Datasets & Models

- **URL**: Papers: https://arxiv.org/abs/2010.03303 | Tools: BoDeGiC, RABBIT/BIMBAS
- **What it contains**: Ground-truth datasets of 5,000+ GitHub accounts (527 confirmed bots) with classification models. BoDeGiC detects bots in git repos based on commit messages. RABBIT classifies bots based on GitHub activity sequences. Dataset of 6,922 git contributors with human/bot labels.
- **How to measure AI**: Distinguish AI-agent-created commits/PRs from human ones. Foundational for filtering bot activity in any analysis. Precision ~0.80 on commit message classification.
- **Access**: Free. Academic datasets and open-source tools.
- **Status**: Active research area. Models may need retraining for newer AI agent patterns.

---

## 7. Academic Datasets & Benchmarks

### 7.1 SWE-bench / SWE-bench Verified / SWE-bench Pro

- **URL**: https://github.com/SWE-bench/SWE-bench | https://swebench.com
- **What it contains**: 2,294 real GitHub issues + pull requests from popular Python repos. Each instance has a GitHub issue, the resolving PR, and fail-to-pass tests. SWE-bench Verified: 500 human-validated samples. SWE-bench Pro: harder long-horizon tasks.
- **How to measure AI**: Industry-standard benchmark for AI coding agents. Leaderboard tracks progress of AI systems at resolving real software issues. Directly measures AI's capability to do real engineering work.
- **Access**: Free. Open source. Leaderboard at swebench.com.
- **Status**: Active. Multiple variants. Community contributions ongoing.

### 7.2 The Stack v2 (BigCode / Hugging Face)

- **URL**: https://huggingface.co/datasets/bigcode/the-stack-v2
- **What it contains**: 67.5 TB of source code in 600+ programming languages with permissive licenses. Derived from Software Heritage archive. Grouped by repository for context-aware training. Used to train StarCoder2.
- **How to measure AI**: Understand the training data behind code LLMs. Analyze what open-source code AI models have been trained on. Compare AI output against training distribution.
- **Access**: Free. Hugging Face Datasets. Opt-out mechanism for contributors.
- **Status**: Active. Maintained by BigCode project (Hugging Face + ServiceNow).

### 7.3 DevGPT Dataset

- **URL**: https://arxiv.org/abs/2309.03914 | Published at MSR 2024
- **What it contains**: 29,778 ChatGPT prompts and responses, including 19,106 code snippets, linked to corresponding GitHub artifacts (commits, issues, PRs, discussions) and Hacker News threads.
- **How to measure AI**: Directly studies how developers interact with ChatGPT for coding. Enables analysis of what tasks developers delegate to AI, code quality of AI responses, and how AI-generated code enters repositories.
- **Access**: Free. Academic dataset.
- **Status**: Published. Snapshot dataset from 2023.

### 7.4 CodeSearchNet

- **URL**: https://github.com/github/CodeSearchNet
- **What it contains**: 2 million (comment, code) pairs from open source libraries in Python, JavaScript, Ruby, Go, Java, PHP. Benchmark for code retrieval using natural language.
- **How to measure AI**: Baseline for evaluating how well AI understands and retrieves code. Used as foundation for many subsequent code AI benchmarks.
- **Access**: Free. Open source (GitHub + Microsoft Research).
- **Status**: Published 2019. Dataset is static but widely referenced.

### 7.5 CodeXGLUE

- **URL**: https://arxiv.org/abs/2102.04664
- **What it contains**: 10 tasks across 14 datasets for code understanding and generation: code-to-text, text-to-code, code completion, code repair, code translation, clone detection, defect detection, etc.
- **How to measure AI**: Comprehensive benchmark suite for evaluating AI code capabilities across the full spectrum of software engineering tasks.
- **Access**: Free. Open source.
- **Status**: Published. Foundational benchmark.

### 7.6 BigCodeBench

- **URL**: https://github.com/bigcode-project/bigcodebench
- **What it contains**: 1,140 software-engineering-oriented programming tasks with complex instructions and diverse function calls. Published at ICLR 2025.
- **How to measure AI**: Evaluates LLMs on realistic SE tasks rather than competitive programming. More representative of actual developer work.
- **Access**: Free. Open source.
- **Status**: Active.

### 7.7 LeetCode Temporal Dataset

- **URL**: https://arxiv.org/abs/2504.14655
- **What it contains**: Temporal dataset of 3,505 LeetCode problems with submission data across multiple time periods. Designed to address data contamination concerns in LLM evaluation.
- **How to measure AI**: Evaluates whether AI models are memorizing vs. genuinely solving problems. Tracks AI performance trends over time on competitive programming.
- **Access**: Free. Academic dataset.
- **Status**: Published 2025.

### 7.8 CommitBench

- **URL**: https://zenodo.org/records/10497442
- **What it contains**: 1,664,590 commit message examples across 6 languages (Java, Python, Go, JavaScript, PHP, Ruby). Privacy and license-aware. Bot commits filtered out.
- **How to measure AI**: Benchmark for commit message generation by AI. Can compare AI-generated commit messages against human baselines.
- **Access**: Free. Available on Zenodo.
- **Status**: Published.

### 7.9 Project CodeNet

- **URL**: https://github.com/IBM/Project_CodeNet
- **What it contains**: 14 million code samples across 55 programming languages from online judge platforms (AIZU, AtCoder). Includes problem descriptions, test cases, and metadata.
- **How to measure AI**: Large-scale benchmark for code generation, translation, and classification tasks.
- **Access**: Free. Open source from IBM Research.
- **Status**: Published 2021.

### 7.10 BenchScout (AI4SE Benchmark Search Tool)

- **URL**: https://arxiv.org/abs/2503.05860
- **What it contains**: Catalog of 273 AI-for-Software-Engineering benchmarks identified from 247 studies since 2014. Semantic search tool for finding appropriate benchmarks.
- **How to measure AI**: Meta-resource for finding the right benchmark for any specific AI-in-SE measurement need.
- **Access**: Free. Academic paper + search tool.
- **Status**: Published 2025.

---

## 8. Code Quality & CI/CD Metrics

### 8.1 OpenSSF Scorecard

- **URL**: https://scorecard.dev/ | https://github.com/ossf/scorecard
- **What it contains**: Automated security health scoring for open source projects. 18 checks including branch protection, CI tests, dependency updates, code review, signed releases. Weekly scans of 1M+ most critical OSS projects. Results in BigQuery: `openssf:scorecardcron.scorecard-v2`.
- **How to measure AI**: Compare security posture of AI-heavy vs. traditional projects. Track whether AI-generated code affects security scores. Analyze trends in code review practices as AI adoption increases.
- **Access**: Free. API, CLI, and BigQuery public dataset.
- **Status**: Active. Maintained by OpenSSF (Linux Foundation).

### 8.2 Codecov

- **URL**: https://about.codecov.io/ | API: https://docs.codecov.com/
- **What it contains**: Code coverage data for open source projects. Historical coverage trends, PR-level coverage changes, flag-based coverage tracking. API v2 provides programmatic access to coverage data and trends.
- **How to measure AI**: Track whether AI-assisted development is changing test coverage patterns. Compare coverage trends in projects with high AI adoption vs. low.
- **Access**: Free for open source projects. API available.
- **Status**: Active.

### 8.3 GitClear (DORA + Code Quality)

- **URL**: https://www.gitclear.com/free_code_quality_2024_dora_report
- **What it contains**: Free code quality and DORA metrics reports for individual repositories. Measures code churn, refactoring rate, code movement, and standard DORA metrics.
- **How to measure AI**: Generate before/after reports for projects adopting AI tools. Track code churn and quality metrics that correlate with AI-assisted development.
- **Access**: Free tier available for individual repos. Paid plans for organizations.
- **Status**: Active.

---

## 9. Developer Community & Sentiment Data

### 9.1 Stack Overflow Data Dump

- **URL**: https://archive.org/details/stackexchange
- **What it contains**: Complete data dump of all Stack Overflow questions, answers, comments, users, votes, and tags. Updated periodically. Millions of posts spanning 15+ years.
- **How to measure AI**: Track question volume trends (declining since AI chatbot adoption), analyze questions about AI coding tools, measure sentiment in answers about AI-generated code, track tag popularity shifts.
- **Access**: Free. Available on Internet Archive under CC BY-SA.
- **Status**: Active. Regular updates.

### 9.2 Stack Exchange Data Explorer (SEDE)

- **URL**: https://data.stackexchange.com/
- **What it contains**: SQL query interface to Stack Exchange databases. Query any Stack Overflow data including questions, answers, users, tags, votes, comments.
- **How to measure AI**: Run custom queries to track AI-related question trends, answer patterns, and user behavior changes.
- **Access**: Free. No authentication needed.
- **Status**: Active.

### 9.3 Hacker News API & Datasets

- **URL**: https://github.com/HackerNews/API | Dataset: https://console.cloud.google.com/marketplace/product/y-combinator/hacker-news
- **What it contains**: All Hacker News stories, comments, jobs, polls, and user data. Also available as BigQuery public dataset (`bigquery-public-data.hacker_news.*`). From 2006 to present.
- **How to measure AI**: Sentiment analysis of developer opinions on AI tools. Track story frequency and engagement for AI coding topics over time. Analyze hiring posts mentioning AI tools. The HN community provides a "real-time barometer of developer excitement."
- **Access**: Free. REST API (no auth needed). BigQuery (1 TB/month free).
- **Status**: Active.

### 9.4 Reddit Data (r/programming, r/ExperiencedDevs, etc.)

- **URL**: https://www.reddit.com/dev/api/ | Archives: various third-party
- **What it contains**: Developer discussions on subreddits like r/programming, r/ExperiencedDevs, r/cscareerquestions, r/ChatGPTCoding, r/cursor, r/ClaudeAI. Comment sentiment, post frequency, upvote patterns.
- **How to measure AI**: Track developer sentiment about AI tools over time. Measure community growth of AI-tool-specific subreddits. Analyze adoption narratives and pain points.
- **Access**: Reddit API has become more restrictive. Free tier available with rate limits. Pushshift archive was restricted but academic access may be available.
- **Status**: Active but API access more limited since 2023 policy changes.

### 9.5 Hacker News Sentiment Analysis Dataset

- **URL**: https://cubig.ai/store/products/586/hacker-news-sentiment-analysis-dataset
- **What it contains**: Pre-computed emotional analysis (polarity, subjectivity, emotional categories) of top Hacker News posts with metadata (title, URL, points, comment count).
- **How to measure AI**: Ready-to-use sentiment data for tracking developer attitudes toward AI over time.
- **Access**: Available via Azoo AI marketplace.
- **Status**: Available.

---

## 10. Job Market & Skills Data

### 10.1 LinkedIn Economic Graph / AI Reports

- **URL**: https://economicgraph.linkedin.com/ | AI reports: https://economicgraph.linkedin.com/research/ai-skills-resources
- **What it contains**: Data from 1B+ LinkedIn members, 41,000 skills, 67M companies. AI Talent Index measuring AI talent by country/industry. AI skills split into AI Engineering and AI Literacy (41,000+ skills tracked). Fastest growing 2024 skills: Custom GPTs, AI Productivity, AI Agents. Work Change Reports on AI job disruption.
- **How to measure AI**: Track AI skills adoption among software engineers. Measure how job postings increasingly require AI proficiency. Analyze which industries are adopting AI engineering talent.
- **Access**: Summary reports are free. Raw data is not publicly available (LinkedIn sells data products). OECD partners get some data access.
- **Status**: Active. Regular report publications.

### 10.2 Indeed / Glassdoor Job Market Data

- **URL**: https://www.hiringlab.org/ (Indeed) | https://www.glassdoor.com/research/
- **What it contains**: Job posting trends, salary data, skills requirements. Track mentions of AI tools (Copilot, Cursor, etc.) in developer job postings.
- **How to measure AI**: Monitor how quickly employers are requiring or preferring AI tool proficiency. Track salary premiums for AI-skilled developers.
- **Access**: Summary reports are free. Detailed data is proprietary.
- **Status**: Active.

---

## 11. Web Archives & Trend Data

### 11.1 Google Trends

- **URL**: https://trends.google.com/
- **What it contains**: Relative search interest over time for any query. Geographic breakdowns. Related queries and topics. Compare up to 5 terms.
- **How to measure AI**: Track search interest for "GitHub Copilot", "Cursor AI", "Claude Code", "AI coding", "vibe coding" over time. Compare adoption signals across tools and regions.
- **Access**: Free. API available (unofficial). Data exportable as CSV.
- **Status**: Active.

### 11.2 Common Crawl

- **URL**: https://commoncrawl.org/
- **What it contains**: Petabyte-scale web crawl data collected since 2008. Contains billions of web pages including developer blogs, documentation sites, forums.
- **How to measure AI**: Analyze web content mentioning AI coding tools. Track documentation references to AI-generated code. Measure how AI tool mentions propagate across the web.
- **Access**: Free. Hosted on AWS as a public dataset. Index is ~21GB compressed.
- **Status**: Active. Monthly crawls.

### 11.3 Internet Archive / Wayback Machine

- **URL**: https://archive.org/ | API: https://web.archive.org/
- **What it contains**: Historical snapshots of websites. 800B+ web pages archived. Includes historical snapshots of code hosting sites, package registry pages, documentation.
- **How to measure AI**: Track historical changes in project documentation mentioning AI, README files referencing AI tools, and website evolution of AI coding tool vendors.
- **Access**: Free. API available.
- **Status**: Active (though funding challenges periodically arise).

---

## 12. Security & Vulnerability Data

### 12.1 Snyk Vulnerability Database

- **URL**: https://security.snyk.io/
- **What it contains**: 31,267+ reported vulnerabilities from GitHub Advisory Database and Snyk.io covering 14,675 open-source repositories across 10 programming languages and 8 package managers (2017-2025).
- **How to measure AI**: Track whether AI-generated code introduces new vulnerability patterns. Analyze correlation between AI adoption periods and vulnerability discovery rates. 59% of teams worry about AI-introduced vulnerabilities.
- **Access**: Database is browsable for free. API requires Snyk account.
- **Status**: Active.

### 12.2 OSV (Open Source Vulnerabilities)

- **URL**: https://osv.dev/ | API: https://osv.dev/docs/
- **What it contains**: Aggregated vulnerability database for open source packages across multiple ecosystems. Used by deps.dev.
- **How to measure AI**: Cross-reference vulnerability data with AI library adoption to assess security implications of AI dependency chains.
- **Access**: Free. Open API.
- **Status**: Active. Maintained by Google.

### 12.3 GitHub Advisory Database

- **URL**: https://github.com/advisories | https://github.com/github/advisory-database
- **What it contains**: Community-curated database of CVEs and security advisories for open source packages.
- **How to measure AI**: Track advisories related to AI libraries. Monitor security issues in AI-generated code patterns.
- **Access**: Free. API via GitHub GraphQL. Raw data in git repository.
- **Status**: Active.

---

## 13. AI Tool Vendor Metrics

### 13.1 GitHub Copilot Public Statistics

- **URL**: Various press releases and blog posts
- **What it contains**: 20 million cumulative users (July 2025), 1.3M paid subscribers, ~50% of code written by Copilot for active users, 88% of generated characters kept, 30-31% suggestion acceptance rate, 90% of developers have committed Copilot code, ~1.2M agent PRs/month, 90% of Fortune 100 using it.
- **How to measure AI**: Baseline adoption and usage statistics for the market-leading AI coding tool.
- **Access**: Free (press releases and blog posts). No API for public aggregate data.
- **Status**: Active. Metrics updated periodically via announcements.

### 13.2 Cursor AI Growth Data

- **URL**: Various press coverage (e.g., https://opsera.ai/blog/cursor-ai-adoption-trends-real-data-from-the-fastest-growing-coding-tool/)
- **What it contains**: Revenue growth ($1M 2023 -> $100M 2024 -> $500M+ ARR), 1M+ users, 360K paying, ~18% market share among paid AI coding tools. Used primarily with Claude models.
- **How to measure AI**: Track the fastest-growing new entrant in AI coding tools. Proxy for developer willingness to switch IDEs for better AI.
- **Access**: Public press coverage and blog analysis. No official public metrics API.
- **Status**: Active. Rapid growth.

### 13.3 VS Code Marketplace Extension Data

- **URL**: https://marketplace.visualstudio.com/ | API: https://marketplace.visualstudio.com/_apis/
- **What it contains**: Install counts, ratings, reviews for all VS Code extensions. GitHub Copilot extension has 10M+ installs. Other AI extensions (Cody, Tabnine, Continue, etc.) have public install counts.
- **How to measure AI**: Track install trends for AI coding extensions over time. Compare growth rates across tools. Monitor user ratings and review sentiment.
- **Access**: Free. Undocumented API available.
- **Status**: Active.

### 13.4 AI Coding Tool Market Reports

- **URL**: Various (Gartner, Forrester, IDC)
- **What it contains**: AI code generation market projected at $6.7B (2024) -> $25.7B (2030). Total AI coding assistant market $8.14B (2025) -> $127B by 2032 (48.1% CAGR). 77% of engineering leaders cite AI integration as a challenge.
- **How to measure AI**: Market sizing provides economic context for adoption trends. Revenue data validates survey-based adoption claims.
- **Access**: Analyst reports are generally paid ($2,000-$10,000+). Press releases with key figures are free.
- **Status**: Active. Updated periodically.

---

## Summary: Most Actionable Data Sources

For a project like claude-code-tracker, the most immediately actionable sources ranked by signal quality and accessibility:

| Priority | Source | Signal Type | Access |
|----------|--------|------------|--------|
| 1 | GitHub Search API (commits) | Direct AI attribution in commits | Free |
| 2 | GH Archive + BigQuery | Historical commit volume (pre-Oct 2025) | Free |
| 3 | PyPI BigQuery Dataset | AI library download trends | Free |
| 4 | npm Download API | AI package adoption in JS/TS | Free |
| 5 | GitHub Innovation Graph | Macro trends (repos, pushes, languages) | Free |
| 6 | deps.dev BigQuery | Cross-ecosystem dependency analysis | Free |
| 7 | Stack Overflow Survey Data | Developer adoption self-reports | Free |
| 8 | Libraries.io Dataset | Cross-registry AI library tracking | Free |
| 9 | OpenSSF Scorecard BigQuery | Security impact of AI-era code | Free |
| 10 | Hacker News BigQuery | Developer sentiment over time | Free |
| 11 | GitHub Octoverse | Platform-level adoption metrics | Free |
| 12 | JetBrains Survey | Detailed AI tool usage breakdowns | Free |
| 13 | DORA Report | AI impact on delivery performance | Free |
| 14 | Software Heritage | Source code content analysis | Free |
| 15 | SWE-bench Leaderboard | AI agent capability tracking | Free |

---

## Sources

- [METR AI Productivity Study](https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/)
- [Stack Overflow Developer Survey 2025](https://survey.stackoverflow.co/2025)
- [JetBrains State of Developer Ecosystem 2025](https://devecosystem-2025.jetbrains.com)
- [GitHub Octoverse 2025](https://octoverse.github.com/)
- [GitHub Innovation Graph](https://innovationgraph.github.com)
- [DORA State of AI-assisted Software Development 2025](https://dora.dev/research/2025/dora-report/)
- [Postman State of the API 2025](https://www.postman.com/state-of-api/2025/)
- [Jellyfish AI Metrics Review](https://jellyfish.co/blog/2025-ai-metrics-in-review/)
- [GitClear Code Quality Research 2025](https://www.gitclear.com/ai_assistant_code_quality_2025_research)
- [GHTorrent](https://ghtorrent.org/)
- [Software Heritage](https://www.softwareheritage.org/)
- [World of Code](https://worldofcode.org/)
- [OSS Insight](https://ossinsight.io/)
- [CNCF DevStats](https://devstats.cncf.io/)
- [OpenDigger](https://open-digger.cn/en/)
- [GrimoireLab / CHAOSS](https://chaoss.github.io/grimoirelab/)
- [Libraries.io](https://libraries.io/)
- [deps.dev / Open Source Insights](https://deps.dev/)
- [PyPI BigQuery Dataset](https://docs.pypi.org/api/bigquery/)
- [npm Download Counts API](https://github.com/npm/registry/blob/main/docs/download-counts.md)
- [OpenSSF Scorecard](https://scorecard.dev/)
- [SWE-bench](https://github.com/SWE-bench/SWE-bench)
- [BigCode / The Stack v2](https://huggingface.co/datasets/bigcode/the-stack-v2)
- [DevGPT Dataset](https://arxiv.org/abs/2309.03914)
- [CodeSearchNet](https://github.com/github/CodeSearchNet)
- [Snyk Security Database](https://security.snyk.io)
- [LinkedIn Economic Graph](https://economicgraph.linkedin.com/)
- [Hacker News BigQuery Dataset](https://console.cloud.google.com/marketplace/product/y-combinator/hacker-news)
- [BenchScout AI4SE Benchmarks](https://arxiv.org/abs/2503.05860)
- [GitHub Copilot Metrics](https://docs.github.com/en/copilot/concepts/copilot-metrics)
- [SonarQube AI Code Detection](https://www.sonarsource.com/blog/auto-detect-and-review-ai-generated-code-from-github-copilot/)
- [Bot Detection in GitHub](https://arxiv.org/abs/2010.03303)
- [Cursor AI Adoption Data](https://opsera.ai/blog/cursor-ai-adoption-trends-real-data-from-the-fastest-growing-coding-tool/)
- [Google BigQuery Public Datasets](https://docs.cloud.google.com/bigquery/public-data)
